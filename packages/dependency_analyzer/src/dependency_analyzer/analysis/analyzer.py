from __future__ import annotations
import networkx as nx
import loguru as lg # type: ignore
from typing import List, Dict, Set, Optional, Generator, Tuple
import re

# Assuming plsql_analyzer is a package accessible in the Python path.
# This import might be needed if we directly access PLSQL_CodeObject attributes like type.
from plsql_analyzer.core.code_object import CodeObjectType # type: ignore


def find_unused_objects(graph: nx.DiGraph, logger: lg.Logger) -> Set[str]:
    """
    Identifies code objects that are not called by any other object within the analyzed scope.

    These are nodes with an in-degree of 0. "Unused" means not referenced by other
    PL/SQL objects within the provided graph. They might still be entry points
    called externally or represent dead code.

    Args:
        graph: The NetworkX DiGraph generated by GraphConstructor.
               Nodes are expected to be object IDs (strings).
        logger: A Loguru logger instance for logging messages.

    Returns:
        A set of node IDs (object IDs) that are unused (in-degree of 0).
    """
    logger.info("Attempting to find unused objects (nodes with in-degree 0)...")
    if not graph:
        logger.warning("Graph is empty or None. Cannot find unused objects.")
        return set()

    unused_nodes: Set[str] = set()
    for node_id in graph.nodes():
        if graph.in_degree(node_id) == 0:
            unused_nodes.add(node_id)
            logger.trace(f"Node '{node_id}' has in-degree 0, marking as unused.")

    logger.info(f"Found {len(unused_nodes)} unused objects: {unused_nodes if unused_nodes else 'None'}")
    return unused_nodes


def find_circular_dependencies(graph: nx.DiGraph, logger: lg.Logger) -> List[List[str]]:
    """
    Detects circular dependencies (cycles) among code objects in the graph.

    Args:
        graph: The NetworkX DiGraph generated by GraphConstructor.
        logger: A Loguru logger instance for logging messages.

    Returns:
        A list of lists, where each inner list contains the node IDs forming a cycle.
        Returns an empty list if no cycles are found or the graph is empty.
    """
    logger.info("Attempting to find circular dependencies...")
    if not graph:
        logger.warning("Graph is empty or None. Cannot find circular dependencies.")
        return []

    try:
        # simple_cycles returns a generator of lists of nodes
        cycles_generator: Generator[List[str], None, None] = nx.simple_cycles(graph)
        cycles: List[List[str]] = list(cycles_generator)

        if cycles:
            logger.info(f"Found {len(cycles)} circular dependencies.")
            for i, cycle in enumerate(cycles):
                logger.debug(f"  Cycle {i+1}: {' -> '.join(cycle)} -> {cycle[0]}")
        else:
            logger.info("No circular dependencies found.")
        return cycles
    except Exception as e:
        logger.error(f"Error finding circular dependencies: {e}", exc_info=True)
        return []


def generate_subgraph_for_node(
    graph: nx.DiGraph,
    node_id: str,
    logger: lg.Logger,
    upstream_depth: int = 1,
    downstream_depth: Optional[int] = None
) -> Optional[nx.DiGraph]:
    """
    Generates a subgraph centered around a specific node, including its upstream
    dependencies (callers) up to a specified depth, and all downstream dependents (callees)
    to the last possible node by default (unless downstream_depth is set).

    Args:
        graph: The NetworkX DiGraph from GraphConstructor.
        node_id: The ID of the central node for the subgraph.
        logger: A Loguru logger instance.
        upstream_depth: How many levels of callers to include (default: 1).
        downstream_depth: How many levels of callees to include. If None (default),
                         includes all reachable downstream nodes.

    Returns:
        A new nx.DiGraph representing the subgraph, or None if the node_id
        is not found or an error occurs.
    """
    logger.info(f"Generating subgraph for node '{node_id}' (upstream_depth={upstream_depth}, downstream_depth={'ALL' if downstream_depth is None else downstream_depth}).")
    if not graph:
        logger.warning("Graph is empty or None. Cannot generate subgraph.")
        return None
    if node_id not in graph:
        logger.warning(f"Node '{node_id}' not found in the graph. Cannot generate subgraph.")
        return None

    nodes_for_subgraph: Set[str] = {node_id}

    # --- Upstream Traversal (limited by upstream_depth) ---
    nodes_to_expand_in_current_level: Set[str] = {node_id}
    for i in range(upstream_depth):
        if not nodes_to_expand_in_current_level:
            logger.trace(f"Upstream level {i+1} for '{node_id}': no nodes to expand from, stopping traversal.")
            break

        nodes_discovered_in_this_iteration: Set[str] = set()
        edges_to_log_for_this_iteration: Set[str] = set()

        # Phase 1: Discover new nodes and the direct edges that led to them
        for expanding_node in nodes_to_expand_in_current_level:
            for pred_neighbor in graph.predecessors(expanding_node):
                if pred_neighbor not in nodes_for_subgraph: # pred_neighbor is genuinely new to the overall subgraph
                    nodes_discovered_in_this_iteration.add(pred_neighbor)
                    edges_to_log_for_this_iteration.add(f"{pred_neighbor}->{expanding_node}")

        if not nodes_discovered_in_this_iteration:
            logger.trace(f"Upstream level {i+1} for '{node_id}': no new nodes discovered.")
            break # Stop upstream traversal if no new nodes are found

        # Phase 2: Log other relevant edges involving the newly_discovered_in_this_iteration nodes
        # nodes_for_subgraph at this point contains all nodes found *before* this iteration's discoveries
        for newly_discovered_node in nodes_discovered_in_this_iteration:
            # Edges between newly_discovered_node and nodes already in nodes_for_subgraph (before this iteration's update)
            for node_already_in_subgraph in nodes_for_subgraph:
                if graph.has_edge(newly_discovered_node, node_already_in_subgraph):
                    edges_to_log_for_this_iteration.add(f"{newly_discovered_node}->{node_already_in_subgraph}")
                if graph.has_edge(node_already_in_subgraph, newly_discovered_node):
                    edges_to_log_for_this_iteration.add(f"{node_already_in_subgraph}->{newly_discovered_node}")
            
            # Edges between newly_discovered_node and OTHER nodes discovered in THIS SAME iteration
            for another_newly_discovered_node in nodes_discovered_in_this_iteration:
                if newly_discovered_node == another_newly_discovered_node:
                    continue
                if graph.has_edge(newly_discovered_node, another_newly_discovered_node):
                    edges_to_log_for_this_iteration.add(f"{newly_discovered_node}->{another_newly_discovered_node}")
                # The reverse (another_newly_discovered_node -> newly_discovered_node) will be covered
                # when another_newly_discovered_node is the outer loop variable, if it exists.

        logger.trace(f"Upstream level {i+1} for '{node_id}': added nodes {nodes_discovered_in_this_iteration if nodes_discovered_in_this_iteration else 'none'}.")
        logger.trace(f"Upstream level {i+1} for '{node_id}': relevant edges for this level {edges_to_log_for_this_iteration if edges_to_log_for_this_iteration else 'none'}.")

        nodes_for_subgraph.update(nodes_discovered_in_this_iteration)
        nodes_to_expand_in_current_level = nodes_discovered_in_this_iteration


    # --- Downstream Traversal (to all reachable nodes if downstream_depth is None) ---
    nodes_to_expand_in_current_level = {node_id} # Reset for downstream, starting from the original node
    level = 0
    while True:
        if downstream_depth is not None and level >= downstream_depth:
            break
        if not nodes_to_expand_in_current_level:
            logger.trace(f"Downstream level {level+1} for '{node_id}': no nodes to expand from, stopping traversal.")
            break

        nodes_discovered_in_this_iteration: Set[str] = set()
        edges_to_log_for_this_iteration: Set[str] = set()

        # Phase 1: Discover new nodes and the direct edges that led to them
        for expanding_node in nodes_to_expand_in_current_level:
            for succ_neighbor in graph.successors(expanding_node):
                if succ_neighbor not in nodes_for_subgraph: # succ_neighbor is genuinely new to the overall subgraph
                    nodes_discovered_in_this_iteration.add(succ_neighbor)
                    edges_to_log_for_this_iteration.add(f"{expanding_node}->{succ_neighbor}")
        
        if not nodes_discovered_in_this_iteration:
            logger.trace(f"Downstream level {level+1} for '{node_id}': no new nodes discovered.")
            break # Stop downstream traversal if no new nodes are found

        # Phase 2: Log other relevant edges involving the newly_discovered_in_this_iteration nodes
        # nodes_for_subgraph at this point contains all nodes found *before* this iteration's discoveries (including upstream ones)
        for newly_discovered_node in nodes_discovered_in_this_iteration:
            # Edges between newly_discovered_node and nodes already in nodes_for_subgraph (before this iteration's update)
            for node_already_in_subgraph in nodes_for_subgraph:
                if graph.has_edge(newly_discovered_node, node_already_in_subgraph):
                    edges_to_log_for_this_iteration.add(f"{newly_discovered_node}->{node_already_in_subgraph}")
                if graph.has_edge(node_already_in_subgraph, newly_discovered_node):
                    edges_to_log_for_this_iteration.add(f"{node_already_in_subgraph}->{newly_discovered_node}")

            # Edges between newly_discovered_node and OTHER nodes discovered in THIS SAME iteration
            for another_newly_discovered_node in nodes_discovered_in_this_iteration:
                if newly_discovered_node == another_newly_discovered_node:
                    continue
                if graph.has_edge(newly_discovered_node, another_newly_discovered_node):
                    edges_to_log_for_this_iteration.add(f"{newly_discovered_node}->{another_newly_discovered_node}")
                # The reverse (another_newly_discovered_node -> newly_discovered_node) will be covered
                # when another_newly_discovered_node is the outer loop variable, if it exists.

        logger.trace(f"Downstream level {level+1} for '{node_id}': added nodes {nodes_discovered_in_this_iteration if nodes_discovered_in_this_iteration else 'none'}.")
        logger.trace(f"Downstream level {level+1} for '{node_id}': relevant edges for this level {edges_to_log_for_this_iteration if edges_to_log_for_this_iteration else 'none'}.")

        nodes_for_subgraph.update(nodes_discovered_in_this_iteration)
        nodes_to_expand_in_current_level = nodes_discovered_in_this_iteration
        level += 1

    if not nodes_for_subgraph:
        logger.warning(f"No nodes identified for subgraph of '{node_id}'. This is unexpected.")
        return None

    logger.info(f"Subgraph for '{node_id}' will contain {len(nodes_for_subgraph)} nodes.")
    # Create the subgraph from the original graph, preserving original node/edge attributes
    subgraph: nx.DiGraph = graph.subgraph(nodes_for_subgraph).copy()
    logger.debug(f"Subgraph for '{node_id}' created with {subgraph.number_of_nodes()} nodes and {subgraph.number_of_edges()} edges.")
    return subgraph


def find_entry_points(graph: nx.DiGraph, logger: lg.Logger) -> Set[str]:
    """
    Identifies potential entry points into the application or system.
    These are nodes with an in-degree of 0, meaning they are not called by
    any other object within the analyzed set.

    Args:
        graph: The NetworkX DiGraph from GraphConstructor.
        logger: A Loguru logger instance.

    Returns:
        A set of node IDs that are potential entry points.
    """
    logger.info("Attempting to find entry points (nodes with in-degree 0)...")
    # This is functionally the same as find_unused_objects but with a different semantic interpretation.
    # We can reuse the logic or call it directly if no further distinction is needed at this stage.
    entry_points = find_unused_objects(graph, logger) # Re-logging will occur, can be refined if needed
    logger.info(f"Identified {len(entry_points)} potential entry points: {entry_points if entry_points else 'None'}")
    return entry_points


def find_terminal_nodes(graph: nx.DiGraph, logger: lg.Logger, exclude_placeholders: bool = True) -> Set[str]:
    """
    Identifies terminal nodes in the graph (nodes with an out-degree of 0).
    These are objects that do not call any other objects within the analyzed set,
    or only call out-of-scope objects (if placeholders are not excluded and represent them).

    Args:
        graph: The NetworkX DiGraph from GraphConstructor.
        logger: A Loguru logger instance.
        exclude_placeholders: If True, attempts to filter out nodes that are
                              placeholders for out-of-scope calls (typically of
                              type CodeObjectType.UNKNOWN).

    Returns:
        A set of node IDs that are terminal nodes.
    """
    logger.info(f"Attempting to find terminal nodes (out-degree 0), exclude_placeholders={exclude_placeholders}...")
    if not graph:
        logger.warning("Graph is empty or None. Cannot find terminal nodes.")
        return set()

    terminal_nodes: Set[str] = set()
    for node_id, node_data in graph.nodes(data=True):
        if graph.out_degree(node_id) == 0:
            if exclude_placeholders:
                # Access the 'object' attribute which should be a PLSQL_CodeObject instance
                code_object_instance = node_data.get('object')
                if code_object_instance and hasattr(code_object_instance, 'type'):
                    if code_object_instance.type == CodeObjectType.UNKNOWN:
                        logger.trace(f"Node '{node_id}' has out-degree 0 but is an UNKNOWN type placeholder. Excluding.")
                        continue # Skip this placeholder node
                else:
                    logger.warning(f"Node '{node_id}' has out-degree 0, but its 'object' attribute or 'type' is missing. Cannot determine if placeholder. Including by default.")
            
            terminal_nodes.add(node_id)
            logger.trace(f"Node '{node_id}' has out-degree 0, marking as terminal.")

    logger.info(f"Found {len(terminal_nodes)} terminal nodes: {terminal_nodes if terminal_nodes else 'None'}")
    return terminal_nodes


def get_node_degrees(graph: nx.DiGraph, node_id: str, logger: lg.Logger) -> Optional[Dict[str, int]]:
    """
    Retrieves the in-degree, out-degree, and total degree for a specific node.

    Args:
        graph: The NetworkX DiGraph from GraphConstructor.
        node_id: The ID of the node for which to get degrees.
        logger: A Loguru logger instance.

    Returns:
        A dictionary with keys "in_degree", "out_degree", "total_degree",
        or None if the node_id is not found or graph is empty.
    """
    logger.debug(f"Getting degrees for node '{node_id}'...")
    if not graph:
        logger.warning("Graph is empty or None. Cannot get node degrees.")
        return None
    if node_id not in graph:
        logger.warning(f"Node '{node_id}' not found in the graph. Cannot get degrees.")
        return None

    try:
        in_degree = graph.in_degree(node_id)
        out_degree = graph.out_degree(node_id)
        # Note: For DiGraph, graph.degree(node_id) is sum of in_degree and out_degree.
        total_degree = graph.degree(node_id) 
        
        degrees = {
            "in_degree": in_degree,
            "out_degree": out_degree,
            "total_degree": total_degree
        }
        logger.info(f"Degrees for node '{node_id}': In={in_degree}, Out={out_degree}, Total={total_degree}")
        return degrees
    except Exception as e:
        logger.error(f"Error getting degrees for node '{node_id}': {e}", exc_info=True)
        return None


def find_all_paths(
    graph: nx.DiGraph,
    source_node_id: str,
    target_node_id: str,
    logger: lg.Logger,
    cutoff: Optional[int] = None
) -> Optional[List[List[str]]]:
    """
    Finds all simple paths (no repeated nodes) between a source and a target node.

    Args:
        graph: The NetworkX DiGraph from GraphConstructor.
        source_node_id: The ID of the starting node.
        target_node_id: The ID of the ending node.
        logger: A Loguru logger instance.
        cutoff: Maximum length of paths to consider. If None, all paths are considered.

    Returns:
        A list of paths, where each path is a list of node IDs.
        Returns None if source or target node is not found, or if graph is empty.
        Returns an empty list if no path exists.
    """
    logger.info(f"Finding all paths from '{source_node_id}' to '{target_node_id}' (cutoff={cutoff})...")
    if not graph:
        logger.warning("Graph is empty or None. Cannot find paths.")
        return None
    if source_node_id not in graph:
        logger.warning(f"Source node '{source_node_id}' not found in graph.")
        return None
    if target_node_id not in graph:
        logger.warning(f"Target node '{target_node_id}' not found in graph.")
        return None
    
    if source_node_id == target_node_id:
        logger.info(f"Source and target node are the same ('{source_node_id}'). Path is just the node itself.")
        return [[source_node_id]]

    try:
        paths_generator: Generator[List[str], None, None] = nx.all_simple_paths(
            graph, source=source_node_id, target=target_node_id, cutoff=cutoff
        )
        paths: List[List[str]] = list(paths_generator)

        if paths:
            logger.info(f"Found {len(paths)} paths from '{source_node_id}' to '{target_node_id}'.")
            # for i, path in enumerate(paths):
            #     logger.debug(f"  Path {i+1}: {' -> '.join(path)}") # Can be verbose
        else:
            logger.info(f"No paths found from '{source_node_id}' to '{target_node_id}'.")
        return paths
    except nx.NetworkXNoPath:
        logger.info(f"No path exists between '{source_node_id}' and '{target_node_id}'.")
        return []
    except Exception as e:
        logger.error(f"Error finding paths between '{source_node_id}' and '{target_node_id}': {e}", exc_info=True)
        return None # Indicate error rather than empty list for unexpected issues


def get_connected_components(
    graph: nx.DiGraph,
    logger: lg.Logger,
    strongly_connected: bool = True
) -> List[Set[str]]:
    """
    Finds connected components in the graph.

    Args:
        graph: The NetworkX DiGraph from GraphConstructor.
        logger: A Loguru logger instance.
        strongly_connected: If True (default), finds strongly connected components (SCCs).
                            If False, finds weakly connected components (WCCs).

    Returns:
        A list of sets, where each set contains node IDs belonging to a component.
        Returns an empty list if graph is empty or an error occurs.
    """
    component_type = "strongly" if strongly_connected else "weakly"
    logger.info(f"Finding {component_type} connected components...")
    if not graph:
        logger.warning(f"Graph is empty or None. Cannot find {component_type} connected components.")
        return []

    try:
        if strongly_connected:
            components_generator: Generator[Set[str], None, None] = nx.strongly_connected_components(graph)
        else:
            # Weakly connected components are defined for undirected graphs.
            # NetworkX handles this by implicitly considering the graph as undirected.
            components_generator = nx.weakly_connected_components(graph)
        
        components: List[Set[str]] = [comp for comp in components_generator if comp] # Filter out empty sets if any

        logger.info(f"Found {len(components)} {component_type} connected components.")
        # for i, comp_set in enumerate(components):
        #     logger.debug(f"  {component_type.capitalize()} Component {i+1} (size {len(comp_set)}): {list(comp_set)[:5]}...") # Log sample
        return components
    except Exception as e:
        logger.error(f"Error finding {component_type} connected components: {e}", exc_info=True)
        return []


def calculate_node_complexity_metrics(graph: nx.DiGraph, logger: lg.Logger) -> None:
    """
    Calculates and stores complexity metrics for each PLSQL_CodeObject node in the graph.
    Metrics:
        - loc: Lines of Code (LOC) based on clean_code
        - num_params: Number of parameters (parsed_parameters)
        - num_calls_made: Number of outgoing calls (unique callees in extracted_calls)
        - acc: Approximate Cyclomatic Complexity (heuristic based on control flow keywords)
    Stores metrics as node attributes: 'loc', 'num_params', 'num_calls_made', 'acc'.
    """
    if not graph:
        logger.warning("Graph is empty or None. Cannot calculate complexity metrics.")
        return

    # Decision-point keywords for ACC (case-insensitive, word boundaries)
    # Only count 'if', 'case', 'loop' not preceded by 'end' (with optional whitespace)
    # Python's regex lookbehind must be fixed-width, so we can't use (?<!end\s*)
    # Instead, match all, then filter out those preceded by 'end' and whitespace in post-processing
    keywords = [r'\bif\b', r'\belsif\b', r'\bcase\b', r'\bwhen\b', r'\bloop\b', r'\bfor\b', r'\bwhile\b', r'\bexception\b', r'\bthen\b']
    acc_pattern = re.compile('|'.join(keywords), re.IGNORECASE)

    def is_false_positive(match):
        # Get up to 10 chars before the match
        start = match.start()
        before = obj.clean_code[max(0, start-10):start].lower()
        # Check for 'end' followed by whitespace right before the keyword
        return bool(re.search(r'end\s*$', before))

    for node_id, node_data in graph.nodes(data=True):
        obj = node_data.get('object')
        if obj is None:
            logger.warning(f"Node '{node_id}' missing 'object' attribute. Skipping complexity metrics.")
            continue
        # LOC
        loc = len(obj.clean_code.splitlines()) if obj.clean_code else 0
        # Number of parameters
        num_params = len(obj.parsed_parameters) if hasattr(obj, 'parsed_parameters') and obj.parsed_parameters else 0
        # Number of outgoing calls (unique callees)
        if hasattr(obj, 'extracted_calls') and obj.extracted_calls:
            unique_callees = set(getattr(call, 'call_name', None) for call in obj.extracted_calls if hasattr(call, 'call_name'))
            num_calls_made = len(unique_callees)
        else:
            num_calls_made = 0
        # Approximate Cyclomatic Complexity (ACC)
        if obj.clean_code:
            matches = list(acc_pattern.finditer(obj.clean_code))
            acc_count = sum(1 for m in matches if not is_false_positive(m))
            acc = acc_count + 1
        else:
            acc = 1
        # Store metrics as node attributes
        graph.nodes[node_id]['loc'] = loc
        graph.nodes[node_id]['num_params'] = num_params
        graph.nodes[node_id]['num_calls_made'] = num_calls_made
        graph.nodes[node_id]['acc'] = acc
        logger.debug(f"Node '{node_id}': LOC={loc}, Params={num_params}, Calls={num_calls_made}, ACC={acc}")

def get_descendants(graph: nx.DiGraph, source_node: str, depth_limit: Optional[int] = None) -> Set[str]:
    """
    Returns the set of all descendant nodes (reachable from source_node) in the graph.
    If depth_limit is None, returns all descendants. Otherwise, limits traversal depth.

    Args:
        graph: The NetworkX DiGraph.
        source_node: The node from which to find descendants.
        depth_limit: Optional maximum depth for traversal.

    Returns:
        Set of descendant node IDs (excluding source_node itself).
    """
    if source_node not in graph:
        return set()
    if depth_limit is None:
        return nx.descendants(graph, source_node)
    # BFS tree includes the source node, so remove it
    return set(nx.bfs_tree(graph, source_node, depth_limit=depth_limit).nodes()) - {source_node}


def get_ancestors(graph: nx.DiGraph, target_node: str, depth_limit: Optional[int] = None) -> Set[str]:
    """
    Returns the set of all ancestor nodes (can reach target_node) in the graph.
    If depth_limit is None, returns all ancestors. Otherwise, limits traversal depth.

    Args:
        graph: The NetworkX DiGraph.
        target_node: The node for which to find ancestors.
        depth_limit: Optional maximum depth for traversal.

    Returns:
        Set of ancestor node IDs (excluding target_node itself).
    """
    if target_node not in graph:
        return set()
    if depth_limit is None:
        return nx.ancestors(graph, target_node)
    # Use reversed graph for upstream traversal
    return set(nx.bfs_tree(graph.reverse(copy=False), target_node, depth_limit=depth_limit).nodes()) - {target_node}
def trace_downstream_paths(
    graph: nx.DiGraph,
    source_node: str,
    logger: lg.Logger,
    depth_limit: Optional[int] = None,
    target_node: Optional[str] = None
) -> List[List[str]]:
    """
    Trace all simple execution paths downstream from a selected node in the dependency graph.
    Paths can be limited by depth or traced to a specific target node.

    Args:
        graph: The NetworkX DiGraph representing dependencies.
        source_node: The node from which to start tracing.
        logger: A Loguru logger instance.
        depth_limit: Optional maximum path length (number of edges).
        target_node: Optional target node to trace paths to.

    Returns:
        A list of paths (each path is a list of node IDs). Returns an empty list if no paths found or nodes are missing.
    """
    logger.info(f"Tracing downstream paths from '{source_node}' (depth_limit={depth_limit}, target_node={target_node})...")
    if not graph:
        logger.warning("Graph is empty or None. Cannot trace downstream paths.")
        return []
    if source_node not in graph:
        logger.warning(f"Source node '{source_node}' not found in graph.")
        return []
    if target_node is not None and target_node not in graph:
        logger.warning(f"Target node '{target_node}' not found in graph.")
        return []

    try:
        if target_node:
            # Use all_simple_paths with cutoff if provided
            logger.debug(f"Tracing all simple paths from '{source_node}' to '{target_node}' (cutoff={depth_limit})...")
            paths = list(nx.all_simple_paths(graph, source=source_node, target=target_node, cutoff=depth_limit))
            logger.info(f"Found {len(paths)} paths from '{source_node}' to '{target_node}'.")
            return paths
        else:
            # No target: find all simple paths from source to all reachable nodes (up to depth_limit)
            # Use DFS to enumerate all simple paths up to depth_limit (path length = number of edges)
            def dfs(current_path: List[str], depth: int):
                current_node = current_path[-1]
                if depth_limit is not None and depth >= depth_limit:
                    return
                for neighbor in graph.successors(current_node):
                    if neighbor in current_path:
                        continue  # avoid cycles
                    new_path = current_path + [neighbor]
                    paths_found.append(new_path)
                    dfs(new_path, depth + 1)
            paths_found: List[List[str]] = []
            dfs([source_node], 0)
            logger.info(f"Found {len(paths_found)} downstream paths from '{source_node}'.")
            return paths_found
    except Exception as e:
        logger.error(f"Error tracing downstream paths from '{source_node}': {e}", exc_info=True)
        return []
def classify_nodes(
    graph: nx.DiGraph,
    logger: lg.Logger,
    complexity_metrics_available: bool = False,
    hub_degree_percentile: float = 0.95,
    hub_betweenness_percentile: float = 0.95,
    hub_pagerank_percentile: float = 0.95,
    utility_out_degree_percentile: float = 0.90,
    utility_max_complexity: int = 50,
    orphan_component_max_size: int = 4
) -> None:
    """
    Classifies nodes in the dependency graph into architectural roles: Hubs, Utilities, Orphans, etc.
    Adds a 'node_role' attribute (list of roles) to each node.

    Args:
        graph: The NetworkX DiGraph to classify.
        logger: A Loguru logger instance.
        complexity_metrics_available: If True, use node complexity for utility classification.
        hub_degree_percentile: Percentile for degree threshold (default: 95th).
        hub_betweenness_percentile: Percentile for betweenness threshold (default: 95th).
        hub_pagerank_percentile: Percentile for PageRank threshold (default: 95th).
        utility_out_degree_percentile: Percentile for utility out-degree (default: 90th).
        utility_max_complexity: Max complexity for utility nodes (if available).
        orphan_component_max_size: Max size for a component to be considered orphaned.
    """
    import numpy as np
    logger.info("Classifying nodes by architectural role...")
    # --- Degree metrics ---
    degrees = dict(graph.degree())
    in_degrees = dict(graph.in_degree())
    out_degrees = dict(graph.out_degree())
    # --- Centrality metrics ---
    betweenness = nx.betweenness_centrality(graph, normalized=True)
    pagerank = nx.pagerank(graph)
    # --- Connected components ---
    wccs = list(nx.weakly_connected_components(graph))
    largest_wcc = max(wccs, key=len) if wccs else set()
    # --- Thresholds (percentile-based) ---
    degree_values = np.array(list(degrees.values()))
    betweenness_values = np.array(list(betweenness.values()))
    pagerank_values = np.array(list(pagerank.values()))
    out_degree_values = np.array(list(out_degrees.values()))
    hub_degree_thresh = np.percentile(degree_values, hub_degree_percentile * 100)
    hub_betweenness_thresh = np.percentile(betweenness_values, hub_betweenness_percentile * 100)
    hub_pagerank_thresh = np.percentile(pagerank_values, hub_pagerank_percentile * 100)
    utility_out_degree_thresh = np.percentile(out_degree_values, utility_out_degree_percentile * 100)
    # --- Complexity metrics (if available) ---
    node_complexity = {}
    if complexity_metrics_available:
        for node_id, data in graph.nodes(data=True):
            node_complexity[node_id] = data.get('loc', 0)  # Use 'loc' as a proxy
    # --- Assign roles ---
    for node_id in graph.nodes():
        roles = []
        # Hubs/connectors
        if (
            degrees[node_id] >= hub_degree_thresh or
            betweenness[node_id] >= hub_betweenness_thresh or
            pagerank[node_id] >= hub_pagerank_thresh
        ):
            roles.append('hub')
        # Utility nodes
        if out_degrees[node_id] >= utility_out_degree_thresh:
            if complexity_metrics_available:
                if node_complexity.get(node_id, 0) <= utility_max_complexity:
                    roles.append('utility')
            else:
                roles.append('utility')
        # Orphaned component members
        for comp in wccs:
            if node_id in comp and len(comp) <= orphan_component_max_size and comp != largest_wcc:
                roles.append('orphan_component_member')
                break
        # Entry/terminal points (reuse existing logic)
        if in_degrees[node_id] == 0:
            roles.append('entry_point')
        if out_degrees[node_id] == 0:
            roles.append('terminal_node')
        graph.nodes[node_id]['node_role'] = roles
        logger.debug(f"Node {node_id}: roles={roles}")
    logger.info("Node classification complete.")


# --- Example Usage (Illustrative) ---
if __name__ == '__main__':
    # Basic Logger Setup for Example
    import sys
    example_logger = lg.logger
    example_logger.remove()
    example_logger.add(
        sys.stderr,
        level="DEBUG", # Set to TRACE for more detail, DEBUG for example output
        colorize=True,
        format="<green>{time:HH:mm:ss.SSS}</green> | <level>{level: <8}</level> | <cyan>{name}:{function}:{line}</cyan> - <level>{message}</level>"
    )
    example_logger.info("Running analyzer.py example functions...")

    # --- Mock Graph Setup ---
    # Create a sample graph similar to what GraphConstructor might produce
    mock_graph = nx.DiGraph()

    # Mock PLSQL_CodeObject (minimal for testing type checks)
    class MockPLSQLCodeObject:
        def __init__(self, id: str, type: CodeObjectType = CodeObjectType.PROCEDURE, package_name: Optional[str] = None):
            self.id = id
            self.name = id.split('.')[-1] if '.' in id else id
            self.package_name = package_name
            self.type = type
        def __repr__(self):
            return f"MockObj({self.id}, type={self.type.name})"

    # Nodes: 'pkg.procA', 'pkg.procB', 'pkg.procC', 'standalone_func', 'util.helper', 'external.api' (placeholder)
    nodes_with_data: List[Tuple[str, Dict[str, MockPLSQLCodeObject]]] = [
        ("pkg.procA", {'object': MockPLSQLCodeObject("pkg.procA", CodeObjectType.PROCEDURE, "pkg")}),
        ("pkg.procB", {'object': MockPLSQLCodeObject("pkg.procB", CodeObjectType.PROCEDURE, "pkg")}),
        ("pkg.procC", {'object': MockPLSQLCodeObject("pkg.procC", CodeObjectType.FUNCTION, "pkg")}),
        ("standalone_func", {'object': MockPLSQLCodeObject("standalone_func", CodeObjectType.FUNCTION)}),
        ("util.helper", {'object': MockPLSQLCodeObject("util.helper", CodeObjectType.PROCEDURE, "util")}),
        ("external.api", {'object': MockPLSQLCodeObject("external.api", CodeObjectType.UNKNOWN)}), # Placeholder
        ("unused_proc", {'object': MockPLSQLCodeObject("unused_proc", CodeObjectType.PROCEDURE)}),
        ("entry_point_proc", {'object': MockPLSQLCodeObject("entry_point_proc", CodeObjectType.PROCEDURE)}),
    ]
    mock_graph.add_nodes_from(nodes_with_data)

    # Edges:
    # pkg.procA -> pkg.procB
    # pkg.procB -> pkg.procC
    # pkg.procC -> pkg.procA  (Cycle: A -> B -> C -> A)
    # standalone_func -> pkg.procA
    # standalone_func -> util.helper
    # util.helper -> external.api (calls an unknown/placeholder)
    # entry_point_proc -> pkg.procB
    mock_graph.add_edges_from([
        ("pkg.procA", "pkg.procB"),
        ("pkg.procB", "pkg.procC"),
        ("pkg.procC", "pkg.procA"),
        ("standalone_func", "pkg.procA"),
        ("standalone_func", "util.helper"),
        ("util.helper", "external.api"),
        ("entry_point_proc", "pkg.procB"), # entry_point_proc calls something
    ])
    example_logger.info(f"Mock graph created with {mock_graph.number_of_nodes()} nodes and {mock_graph.number_of_edges()} edges.")

    # --- Test find_unused_objects ---
    example_logger.info("\\n--- Testing find_unused_objects ---")
    unused = find_unused_objects(mock_graph, example_logger)
    # Expected: {'unused_proc'} (entry_point_proc is not unused as it has 0 in-degree but is used as an example entry)
    # Corrected expectation: find_unused_objects finds all with in-degree 0.
    # So, 'standalone_func', 'unused_proc', 'entry_point_proc'
    example_logger.info(f"Unused objects: {unused}")


    # --- Test find_circular_dependencies ---
    example_logger.info("\\n--- Testing find_circular_dependencies ---")
    cycles = find_circular_dependencies(mock_graph, example_logger)
    # Expected: [['pkg.procA', 'pkg.procB', 'pkg.procC']] or permutations
    example_logger.info(f"Circular dependencies: {cycles}")

    # --- Test generate_subgraph_for_node ---
    example_logger.info("\\n--- Testing generate_subgraph_for_node ---")
    subgraph_A = generate_subgraph_for_node(mock_graph, "pkg.procA", example_logger, upstream_depth=1, downstream_depth=1)
    if subgraph_A:
        example_logger.info(f"Subgraph for 'pkg.procA' (1 level up/down): Nodes={list(subgraph_A.nodes())}, Edges={list(subgraph_A.edges())}")
    
    subgraph_standalone = generate_subgraph_for_node(mock_graph, "standalone_func", example_logger, upstream_depth=0, downstream_depth=2)
    if subgraph_standalone:
        example_logger.info(f"Subgraph for 'standalone_func' (0 levels up, 2 levels down): Nodes={list(subgraph_standalone.nodes())}, Edges={list(subgraph_standalone.edges())}")

    # --- Test find_entry_points ---
    example_logger.info("\\n--- Testing find_entry_points ---")
    entries = find_entry_points(mock_graph, example_logger)
    # Expected: {'standalone_func', 'unused_proc', 'entry_point_proc'}
    example_logger.info(f"Potential entry points: {entries}")

    # --- Test find_terminal_nodes ---
    example_logger.info("\\n--- Testing find_terminal_nodes ---")
    terminals_exclude_placeholders = find_terminal_nodes(mock_graph, example_logger, exclude_placeholders=True)
    # Expected: empty set, because external.api is UNKNOWN and procC is in a cycle.
    # If external.api was not UNKNOWN, it would be here.
    # If util.helper didn't call external.api, util.helper would be here.
    # If pkg.procC didn't call pkg.procA, it would be here.
    # The only node with out-degree 0 is 'external.api'. If excluded, then empty.
    example_logger.info(f"Terminal nodes (excluding placeholders): {terminals_exclude_placeholders}")

    terminals_include_placeholders = find_terminal_nodes(mock_graph, example_logger, exclude_placeholders=False)
    # Expected: {'external.api'}
    example_logger.info(f"Terminal nodes (including placeholders): {terminals_include_placeholders}")


    # --- Test get_node_degrees ---
    example_logger.info("\\n--- Testing get_node_degrees ---")
    degrees_A = get_node_degrees(mock_graph, "pkg.procA", example_logger)
    # Expected: {'in_degree': 2 ('pkg.procC', 'standalone_func'), 'out_degree': 1 ('pkg.procB'), 'total_degree': 3}
    example_logger.info(f"Degrees for 'pkg.procA': {degrees_A}")

    degrees_external = get_node_degrees(mock_graph, "external.api", example_logger)
    # Expected: {'in_degree': 1 ('util.helper'), 'out_degree': 0, 'total_degree': 1}
    example_logger.info(f"Degrees for 'external.api': {degrees_external}")

    # --- Test find_all_paths ---
    example_logger.info("\\n--- Testing find_all_paths ---")
    paths_standalone_to_C = find_all_paths(mock_graph, "standalone_func", "pkg.procC", example_logger)
    # Expected: [['standalone_func', 'pkg.procA', 'pkg.procB', 'pkg.procC']]
    example_logger.info(f"Paths from 'standalone_func' to 'pkg.procC': {paths_standalone_to_C}")
    
    paths_A_to_external = find_all_paths(mock_graph, "pkg.procA", "external.api", example_logger, cutoff=3) # Too short
    example_logger.info(f"Paths from 'pkg.procA' to 'external.api' (cutoff 3): {paths_A_to_external}")
    
    paths_A_to_external_long = find_all_paths(mock_graph, "pkg.procA", "external.api", example_logger, cutoff=5)
    # Expected: [['pkg.procA', 'pkg.procB', 'pkg.procC', 'pkg.procA', ... NO, simple paths!
    # Path: pkg.procA -> pkg.procB -> pkg.procC -> (stuck in cycle, cannot reach standalone_func -> util.helper -> external.api without repeating nodes from cycle)
    # Actually, there is no simple path from A to external.api because A is in a cycle and standalone_func is outside pointing in.
    # Let's try standalone_func to external.api
    paths_standalone_to_external = find_all_paths(mock_graph, "standalone_func", "external.api", example_logger)
    # Expected: [['standalone_func', 'util.helper', 'external.api']]
    example_logger.info(f"Paths from 'standalone_func' to 'external.api': {paths_standalone_to_external}")


    # --- Test get_connected_components ---
    example_logger.info("\\n--- Testing get_connected_components ---")
    scc = get_connected_components(mock_graph, example_logger, strongly_connected=True)
    # Expected SCCs:
    # 1. {'pkg.procA', 'pkg.procB', 'pkg.procC'} (the cycle)
    # 2. {'standalone_func'}
    # 3. {'util.helper'}
    # 4. {'external.api'}
    # 5. {'unused_proc'}
    # 6. {'entry_point_proc'}
    example_logger.info(f"Strongly Connected Components ({len(scc)}):")
    for i, comp in enumerate(scc):
        example_logger.info(f"  SCC {i+1}: {comp}")

    wcc = get_connected_components(mock_graph, example_logger, strongly_connected=False)
    # Expected WCCs:
    # 1. {'pkg.procA', 'pkg.procB', 'pkg.procC', 'standalone_func', 'util.helper', 'external.api', 'entry_point_proc'} (all connected if directions ignored)
    # 2. {'unused_proc'} (isolated)
    example_logger.info(f"Weakly Connected Components ({len(wcc)}):")
    for i, comp in enumerate(wcc):
        example_logger.info(f"  WCC {i+1}: {comp}")

    # --- Test trace_downstream_paths ---
    example_logger.info("\\n--- Testing trace_downstream_paths ---")
    traced_paths_A = trace_downstream_paths(mock_graph, "pkg.procA", example_logger, depth_limit=2)
    # From pkg.procA, within 2 steps: to pkg.procB, pkg.procC, standalone_func
    # Paths: [['pkg.procA', 'pkg.procB'], ['pkg.procA', 'pkg.procB', 'pkg.procC']]
    example_logger.info(f"Traced downstream paths from 'pkg.procA' (depth 2): {traced_paths_A}")

    traced_paths_standalone = trace_downstream_paths(mock_graph, "standalone_func", example_logger, depth_limit=2)
    # From standalone_func, within 2 steps: to pkg.procA, util.helper
    # Paths: [['standalone_func', 'pkg.procA'], ['standalone_func', 'pkg.procA', 'pkg.procB'], ['standalone_func', 'util.helper'], ['standalone_func', 'util.helper', 'external.api']]
    example_logger.info(f"Traced downstream paths from 'standalone_func' (depth 2): {traced_paths_standalone}")

    traced_paths_to_external = trace_downstream_paths(mock_graph, "standalone_func", example_logger, depth_limit=3, target_node="external.api")
    # From standalone_func to external.api, within 3 steps: util.helper -> external.api
    # Paths: [['standalone_func', 'util.helper', 'external.api']]
    example_logger.info(f"Traced downstream paths to 'external.api' from 'standalone_func' (depth 3): {traced_paths_to_external}")

    example_logger.info("\\nAnalyzer example finished.")
