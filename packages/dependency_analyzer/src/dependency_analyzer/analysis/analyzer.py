from __future__ import annotations
import networkx as nx
import loguru as lg # type: ignore
from typing import List, Dict, Set, Optional, Generator
import re

# Assuming plsql_analyzer is a package accessible in the Python path.
# This import might be needed if we directly access PLSQL_CodeObject attributes like type.
from plsql_analyzer.core.code_object import CodeObjectType # type: ignore


def find_unused_objects(graph: nx.DiGraph, logger: lg.Logger) -> Set[str]:
    """
    Identifies code objects that are not called by any other object within the analyzed scope.

    These are nodes with an in-degree of 0. "Unused" means not referenced by other
    PL/SQL objects within the provided graph. They might still be entry points
    called externally or represent dead code.

    Args:
        graph: The NetworkX DiGraph generated by GraphConstructor.
               Nodes are expected to be object IDs (strings).
        logger: A Loguru logger instance for logging messages.

    Returns:
        A set of node IDs (object IDs) that are unused (in-degree of 0).
    """
    logger.info("Attempting to find unused objects (nodes with in-degree 0)...")
    if not graph:
        logger.warning("Graph is empty or None. Cannot find unused objects.")
        return set()

    unused_nodes: Set[str] = set()
    for node_id in graph.nodes():
        if graph.in_degree(node_id) == 0:
            unused_nodes.add(node_id)
            logger.trace(f"Node '{node_id}' has in-degree 0, marking as unused.")

    logger.info(f"Found {len(unused_nodes)} unused objects: {unused_nodes if unused_nodes else 'None'}")
    return unused_nodes

def find_circular_dependencies(graph: nx.DiGraph, logger: lg.Logger) -> List[List[str]]:
    """
    Detects circular dependencies (cycles) among code objects in the graph.

    Args:
        graph: The NetworkX DiGraph generated by GraphConstructor.
        logger: A Loguru logger instance for logging messages.

    Returns:
        A list of lists, where each inner list contains the node IDs forming a cycle.
        Returns an empty list if no cycles are found or the graph is empty.
    """
    logger.info("Attempting to find circular dependencies...")
    if not graph:
        logger.warning("Graph is empty or None. Cannot find circular dependencies.")
        return []

    try:
        # simple_cycles returns a generator of lists of nodes
        cycles_generator: Generator[List[str], None, None] = nx.simple_cycles(graph)
        cycles: List[List[str]] = list(cycles_generator)

        if cycles:
            logger.info(f"Found {len(cycles)} circular dependencies.")
            for i, cycle in enumerate(cycles):
                logger.debug(f"  Cycle {i+1}: {' -> '.join(cycle)} -> {cycle[0]}")
        else:
            logger.info("No circular dependencies found.")
        return cycles
    except Exception as e:
        logger.error(f"Error finding circular dependencies: {e}", exc_info=True)
        return []

def generate_subgraph_for_node(
    graph: nx.DiGraph,
    node_id: str,
    logger: lg.Logger,
    upstream_depth: int = 0,
    downstream_depth: Optional[int] = None
) -> Optional[nx.DiGraph]:
    """
    Generates a subgraph centered around a specific node, including its upstream
    dependencies (callers) up to a specified depth, and all downstream dependents (callees)
    to the last possible node by default (unless downstream_depth is set).

    Args:
        graph: The NetworkX DiGraph from GraphConstructor.
        node_id: The ID of the central node for the subgraph.
        logger: A Loguru logger instance.
        upstream_depth: How many levels of callers to include (default: 1).
        downstream_depth: How many levels of callees to include. If None (default),
                         includes all reachable downstream nodes.

    Returns:
        A new nx.DiGraph representing the subgraph, or None if the node_id
        is not found or an error occurs.
    """
    logger.info(f"Generating subgraph for node '{node_id}' (upstream_depth={upstream_depth}, downstream_depth={'ALL' if downstream_depth is None else downstream_depth}).")
    if not graph:
        logger.warning("Graph is empty or None. Cannot generate subgraph.")
        return None
    if node_id not in graph:
        logger.warning(f"Node '{node_id}' not found in the graph. Cannot generate subgraph.")
        return None

    nodes_for_subgraph: Set[str] = {node_id}

    # --- Upstream Traversal (limited by upstream_depth) ---
    nodes_to_expand_in_current_level: Set[str] = {node_id}
    for i in range(upstream_depth):
        if not nodes_to_expand_in_current_level:
            logger.trace(f"Upstream level {i+1} for '{node_id}': no nodes to expand from, stopping traversal.")
            break

        nodes_discovered_in_this_iteration: Set[str] = set()
        edges_to_log_for_this_iteration: Set[str] = set()

        # Phase 1: Discover new nodes and the direct edges that led to them
        for expanding_node in nodes_to_expand_in_current_level:
            for pred_neighbor in graph.predecessors(expanding_node):
                if pred_neighbor not in nodes_for_subgraph: # pred_neighbor is genuinely new to the overall subgraph
                    nodes_discovered_in_this_iteration.add(pred_neighbor)
                    edges_to_log_for_this_iteration.add(f"{pred_neighbor}->{expanding_node}")

        if not nodes_discovered_in_this_iteration:
            logger.trace(f"Upstream level {i+1} for '{node_id}': no new nodes discovered.")
            break # Stop upstream traversal if no new nodes are found

        # Phase 2: Log other relevant edges involving the newly_discovered_in_this_iteration nodes
        # nodes_for_subgraph at this point contains all nodes found *before* this iteration's discoveries
        for newly_discovered_node in nodes_discovered_in_this_iteration:
            # Edges between newly_discovered_node and nodes already in nodes_for_subgraph (before this iteration's update)
            for node_already_in_subgraph in nodes_for_subgraph:
                if graph.has_edge(newly_discovered_node, node_already_in_subgraph):
                    edges_to_log_for_this_iteration.add(f"{newly_discovered_node}->{node_already_in_subgraph}")
                if graph.has_edge(node_already_in_subgraph, newly_discovered_node):
                    edges_to_log_for_this_iteration.add(f"{node_already_in_subgraph}->{newly_discovered_node}")
            
            # Edges between newly_discovered_node and OTHER nodes discovered in THIS SAME iteration
            for another_newly_discovered_node in nodes_discovered_in_this_iteration:
                if newly_discovered_node == another_newly_discovered_node:
                    continue
                if graph.has_edge(newly_discovered_node, another_newly_discovered_node):
                    edges_to_log_for_this_iteration.add(f"{newly_discovered_node}->{another_newly_discovered_node}")
                # The reverse (another_newly_discovered_node -> newly_discovered_node) will be covered
                # when another_newly_discovered_node is the outer loop variable, if it exists.

        logger.trace(f"Upstream level {i+1} for '{node_id}': added nodes {nodes_discovered_in_this_iteration if nodes_discovered_in_this_iteration else 'none'}.")
        logger.trace(f"Upstream level {i+1} for '{node_id}': relevant edges for this level {edges_to_log_for_this_iteration if edges_to_log_for_this_iteration else 'none'}.")

        nodes_for_subgraph.update(nodes_discovered_in_this_iteration)
        nodes_to_expand_in_current_level = nodes_discovered_in_this_iteration


    # --- Downstream Traversal (to all reachable nodes if downstream_depth is None) ---
    nodes_to_expand_in_current_level = {node_id} # Reset for downstream, starting from the original node
    level = 0
    while True:
        if downstream_depth is not None and level >= downstream_depth:
            break
        if not nodes_to_expand_in_current_level:
            logger.trace(f"Downstream level {level+1} for '{node_id}': no nodes to expand from, stopping traversal.")
            break

        nodes_discovered_in_this_iteration: Set[str] = set()
        edges_to_log_for_this_iteration: Set[str] = set()

        # Phase 1: Discover new nodes and the direct edges that led to them
        for expanding_node in nodes_to_expand_in_current_level:
            for succ_neighbor in graph.successors(expanding_node):
                if succ_neighbor not in nodes_for_subgraph: # succ_neighbor is genuinely new to the overall subgraph
                    nodes_discovered_in_this_iteration.add(succ_neighbor)
                    edges_to_log_for_this_iteration.add(f"{expanding_node}->{succ_neighbor}")
        
        if not nodes_discovered_in_this_iteration:
            logger.trace(f"Downstream level {level+1} for '{node_id}': no new nodes discovered.")
            break # Stop downstream traversal if no new nodes are found

        # Phase 2: Log other relevant edges involving the newly_discovered_in_this_iteration nodes
        # nodes_for_subgraph at this point contains all nodes found *before* this iteration's discoveries (including upstream ones)
        for newly_discovered_node in nodes_discovered_in_this_iteration:
            # Edges between newly_discovered_node and nodes already in nodes_for_subgraph (before this iteration's update)
            for node_already_in_subgraph in nodes_for_subgraph:
                if graph.has_edge(newly_discovered_node, node_already_in_subgraph):
                    edges_to_log_for_this_iteration.add(f"{newly_discovered_node}->{node_already_in_subgraph}")
                if graph.has_edge(node_already_in_subgraph, newly_discovered_node):
                    edges_to_log_for_this_iteration.add(f"{node_already_in_subgraph}->{newly_discovered_node}")

            # Edges between newly_discovered_node and OTHER nodes discovered in THIS SAME iteration
            for another_newly_discovered_node in nodes_discovered_in_this_iteration:
                if newly_discovered_node == another_newly_discovered_node:
                    continue
                if graph.has_edge(newly_discovered_node, another_newly_discovered_node):
                    edges_to_log_for_this_iteration.add(f"{newly_discovered_node}->{another_newly_discovered_node}")
                # The reverse (another_newly_discovered_node -> newly_discovered_node) will be covered
                # when another_newly_discovered_node is the outer loop variable, if it exists.

        logger.trace(f"Downstream level {level+1} for '{node_id}': added nodes {nodes_discovered_in_this_iteration if nodes_discovered_in_this_iteration else 'none'}.")
        logger.trace(f"Downstream level {level+1} for '{node_id}': relevant edges for this level {edges_to_log_for_this_iteration if edges_to_log_for_this_iteration else 'none'}.")

        nodes_for_subgraph.update(nodes_discovered_in_this_iteration)
        nodes_to_expand_in_current_level = nodes_discovered_in_this_iteration
        level += 1

    if not nodes_for_subgraph:
        logger.warning(f"No nodes identified for subgraph of '{node_id}'. This is unexpected.")
        return None

    logger.info(f"Subgraph for '{node_id}' will contain {len(nodes_for_subgraph)} nodes.")
    # Create the subgraph from the original graph, preserving original node/edge attributes
    subgraph: nx.DiGraph = graph.subgraph(nodes_for_subgraph).copy()
    logger.debug(f"Subgraph for '{node_id}' created with {subgraph.number_of_nodes()} nodes and {subgraph.number_of_edges()} edges.")
    return subgraph

def find_entry_points(graph: nx.DiGraph, logger: lg.Logger) -> Set[str]:
    """
    Identifies potential entry points into the application or system.
    These are nodes with an in-degree of 0, meaning they are not called by
    any other object within the analyzed set.

    Args:
        graph: The NetworkX DiGraph from GraphConstructor.
        logger: A Loguru logger instance.

    Returns:
        A set of node IDs that are potential entry points.
    """
    logger.info("Attempting to find entry points (nodes with in-degree 0)...")
    # This is functionally the same as find_unused_objects but with a different semantic interpretation.
    # We can reuse the logic or call it directly if no further distinction is needed at this stage.
    entry_points = find_unused_objects(graph, logger) # Re-logging will occur, can be refined if needed
    logger.info(f"Identified {len(entry_points)} potential entry points: {entry_points if entry_points else 'None'}")
    return entry_points

def find_terminal_nodes(graph: nx.DiGraph, logger: lg.Logger, exclude_placeholders: bool = True) -> Set[str]:
    """
    Identifies terminal nodes in the graph (nodes with an out-degree of 0).
    These are objects that do not call any other objects within the analyzed set,
    or only call out-of-scope objects (if placeholders are not excluded and represent them).

    Args:
        graph: The NetworkX DiGraph from GraphConstructor.
        logger: A Loguru logger instance.
        exclude_placeholders: If True, attempts to filter out nodes that are
                              placeholders for out-of-scope calls (typically of
                              type CodeObjectType.UNKNOWN).

    Returns:
        A set of node IDs that are terminal nodes.
    """
    logger.info(f"Attempting to find terminal nodes (out-degree 0), exclude_placeholders={exclude_placeholders}...")
    if not graph:
        logger.warning("Graph is empty or None. Cannot find terminal nodes.")
        return set()

    terminal_nodes: Set[str] = set()
    for node_id, node_data in graph.nodes(data=True):
        if graph.out_degree(node_id) == 0:
            if exclude_placeholders:
                # Access the 'object' attribute which should be a PLSQL_CodeObject instance
                code_object_instance = node_data.get('object')
                if code_object_instance and hasattr(code_object_instance, 'type'):
                    if code_object_instance.type == CodeObjectType.UNKNOWN:
                        logger.trace(f"Node '{node_id}' has out-degree 0 but is an UNKNOWN type placeholder. Excluding.")
                        continue # Skip this placeholder node
                else:
                    logger.warning(f"Node '{node_id}' has out-degree 0, but its 'object' attribute or 'type' is missing. Cannot determine if placeholder. Including by default.")
            
            terminal_nodes.add(node_id)
            logger.trace(f"Node '{node_id}' has out-degree 0, marking as terminal.")

    logger.info(f"Found {len(terminal_nodes)} terminal nodes: {terminal_nodes if terminal_nodes else 'None'}")
    return terminal_nodes

def get_node_degrees(graph: nx.DiGraph, node_id: str, logger: lg.Logger) -> Optional[Dict[str, int]]:
    """
    Retrieves the in-degree, out-degree, and total degree for a specific node.

    Args:
        graph: The NetworkX DiGraph from GraphConstructor.
        node_id: The ID of the node for which to get degrees.
        logger: A Loguru logger instance.

    Returns:
        A dictionary with keys "in_degree", "out_degree", "total_degree",
        or None if the node_id is not found or graph is empty.
    """
    logger.debug(f"Getting degrees for node '{node_id}'...")
    if not graph:
        logger.warning("Graph is empty or None. Cannot get node degrees.")
        return None
    if node_id not in graph:
        logger.warning(f"Node '{node_id}' not found in the graph. Cannot get degrees.")
        return None

    try:
        in_degree = graph.in_degree(node_id)
        out_degree = graph.out_degree(node_id)
        # Note: For DiGraph, graph.degree(node_id) is sum of in_degree and out_degree.
        total_degree = graph.degree(node_id) 
        
        degrees = {
            "in_degree": in_degree,
            "out_degree": out_degree,
            "total_degree": total_degree
        }
        logger.info(f"Degrees for node '{node_id}': In={in_degree}, Out={out_degree}, Total={total_degree}")
        return degrees
    except Exception as e:
        logger.error(f"Error getting degrees for node '{node_id}': {e}", exc_info=True)
        return None

def find_all_paths(
    graph: nx.DiGraph,
    source_node_id: str,
    target_node_id: str,
    logger: lg.Logger,
    cutoff: Optional[int] = None
) -> Optional[List[List[str]]]:
    """
    Finds all simple paths (no repeated nodes) between a source and a target node.

    Args:
        graph: The NetworkX DiGraph from GraphConstructor.
        source_node_id: The ID of the starting node.
        target_node_id: The ID of the ending node.
        logger: A Loguru logger instance.
        cutoff: Maximum length of paths to consider. If None, all paths are considered.

    Returns:
        A list of paths, where each path is a list of node IDs.
        Returns None if source or target node is not found, or if graph is empty.
        Returns an empty list if no path exists.
    """
    logger.info(f"Finding all paths from '{source_node_id}' to '{target_node_id}' (cutoff={cutoff})...")
    if not graph:
        logger.warning("Graph is empty or None. Cannot find paths.")
        return None
    if source_node_id not in graph:
        logger.warning(f"Source node '{source_node_id}' not found in graph.")
        return None
    if target_node_id not in graph:
        logger.warning(f"Target node '{target_node_id}' not found in graph.")
        return None
    
    if source_node_id == target_node_id:
        logger.info(f"Source and target node are the same ('{source_node_id}'). Path is just the node itself.")
        return [[source_node_id]]

    try:
        paths_generator: Generator[List[str], None, None] = nx.all_simple_paths(
            graph, source=source_node_id, target=target_node_id, cutoff=cutoff
        )
        paths: List[List[str]] = list(paths_generator)

        if paths:
            logger.info(f"Found {len(paths)} paths from '{source_node_id}' to '{target_node_id}'.")
            # for i, path in enumerate(paths):
            #     logger.debug(f"  Path {i+1}: {' -> '.join(path)}") # Can be verbose
        else:
            logger.info(f"No paths found from '{source_node_id}' to '{target_node_id}'.")
        return paths
    except nx.NetworkXNoPath:
        logger.info(f"No path exists between '{source_node_id}' and '{target_node_id}'.")
        return []
    except Exception as e:
        logger.error(f"Error finding paths between '{source_node_id}' and '{target_node_id}': {e}", exc_info=True)
        return None # Indicate error rather than empty list for unexpected issues

def get_connected_components(
    graph: nx.DiGraph,
    logger: lg.Logger,
    strongly_connected: bool = True
) -> List[Set[str]]:
    """
    Finds connected components in the graph.

    Args:
        graph: The NetworkX DiGraph from GraphConstructor.
        logger: A Loguru logger instance.
        strongly_connected: If True (default), finds strongly connected components (SCCs).
                            If False, finds weakly connected components (WCCs).

    Returns:
        A list of sets, where each set contains node IDs belonging to a component.
        Returns an empty list if graph is empty or an error occurs.
    """
    component_type = "strongly" if strongly_connected else "weakly"
    logger.info(f"Finding {component_type} connected components...")
    if not graph:
        logger.warning(f"Graph is empty or None. Cannot find {component_type} connected components.")
        return []

    try:
        if strongly_connected:
            components_generator: Generator[Set[str], None, None] = nx.strongly_connected_components(graph)
        else:
            # Weakly connected components are defined for undirected graphs.
            # NetworkX handles this by implicitly considering the graph as undirected.
            components_generator = nx.weakly_connected_components(graph)
        
        components: List[Set[str]] = [comp for comp in components_generator if comp] # Filter out empty sets if any

        logger.info(f"Found {len(components)} {component_type} connected components.")
        # for i, comp_set in enumerate(components):
        #     logger.debug(f"  {component_type.capitalize()} Component {i+1} (size {len(comp_set)}): {list(comp_set)[:5]}...") # Log sample
        return components
    except Exception as e:
        logger.error(f"Error finding {component_type} connected components: {e}", exc_info=True)
        return []

def calculate_node_complexity_metrics(graph: nx.DiGraph, logger: lg.Logger) -> nx.DiGraph:
    """
    Calculates and stores complexity metrics for each PLSQL_CodeObject node in the graph.
    Metrics:
        - loc: Lines of Code (LOC) based on clean_code
        - num_params: Number of parameters (parsed_parameters)
        - num_calls_made: Number of outgoing calls (unique callees in extracted_calls)
        - acc: Approximate Cyclomatic Complexity (heuristic based on control flow keywords)
    Stores metrics as node attributes: 'loc', 'num_params', 'num_calls_made', 'acc'.
    """
    if not graph:
        logger.warning("Graph is empty or None. Cannot calculate complexity metrics.")
        return

    # Decision-point keywords for ACC (case-insensitive, word boundaries)
    # Only count 'if', 'case', 'loop' not preceded by 'end' (with optional whitespace)
    # Python's regex lookbehind must be fixed-width, so we can't use (?<!end\s*)
    # Instead, match all, then filter out those preceded by 'end' and whitespace in post-processing
    keywords = [r'\bif\b', r'\belsif\b', r'\bcase\b', r'\bwhen\b', r'\bloop\b', r'\bfor\b', r'\bwhile\b', r'\bexception\b', r'\bthen\b']
    acc_pattern = re.compile('|'.join(keywords), re.IGNORECASE)

    def is_false_positive(match):
        # Get up to 10 chars before the match
        start = match.start()
        before = obj.clean_code[max(0, start-10):start].lower()
        # Check for 'end' followed by whitespace right before the keyword
        return bool(re.search(r'end\s*$', before))

    for node_id, node_data in graph.nodes(data=True):
        obj = node_data.get('object')
        if obj is None:
            logger.warning(f"Node '{node_id}' missing 'object' attribute. Skipping complexity metrics.")
            continue
        # LOC
        loc = len(obj.clean_code.splitlines()) if obj.clean_code else 0
        # Number of parameters
        num_params = len(obj.parsed_parameters) if hasattr(obj, 'parsed_parameters') and obj.parsed_parameters else 0
        # Number of outgoing calls (unique callees)
        if hasattr(obj, 'extracted_calls') and obj.extracted_calls:
            unique_callees = set(getattr(call, 'call_name', None) for call in obj.extracted_calls if hasattr(call, 'call_name'))
            num_calls_made = len(unique_callees)
        else:
            num_calls_made = 0
        # Approximate Cyclomatic Complexity (ACC)
        if obj.clean_code:
            matches = list(acc_pattern.finditer(obj.clean_code))
            acc_count = sum(1 for m in matches if not is_false_positive(m))
            acc = acc_count + 1
        else:
            acc = 1
        # Store metrics as node attributes
        graph.nodes[node_id]['loc'] = loc
        graph.nodes[node_id]['num_params'] = num_params
        graph.nodes[node_id]['num_calls_made'] = num_calls_made
        graph.nodes[node_id]['acc'] = acc
        logger.debug(f"Node '{node_id}': LOC={loc}, Params={num_params}, Calls={num_calls_made}, ACC={acc}")
    
    return graph

def get_descendants(graph: nx.DiGraph, source_node: str, depth_limit: Optional[int] = None) -> Set[str]:
    """
    Returns the set of all descendant nodes (reachable from source_node) in the graph.
    If depth_limit is None, returns all descendants. Otherwise, limits traversal depth.

    Args:
        graph: The NetworkX DiGraph.
        source_node: The node from which to find descendants.
        depth_limit: Optional maximum depth for traversal.

    Returns:
        Set of descendant node IDs (excluding source_node itself).
    """
    if source_node not in graph:
        return set()
    if depth_limit is None:
        return nx.descendants(graph, source_node)
    # BFS tree includes the source node, so remove it
    return set(nx.bfs_tree(graph, source_node, depth_limit=depth_limit).nodes()) - {source_node}

def get_ancestors(graph: nx.DiGraph, target_node: str, depth_limit: Optional[int] = None) -> Set[str]:
    """
    Returns the set of all ancestor nodes (can reach target_node) in the graph.
    If depth_limit is None, returns all ancestors. Otherwise, limits traversal depth.

    Args:
        graph: The NetworkX DiGraph.
        target_node: The node for which to find ancestors.
        depth_limit: Optional maximum depth for traversal.

    Returns:
        Set of ancestor node IDs (excluding target_node itself).
    """
    if target_node not in graph:
        return set()
    if depth_limit is None:
        return nx.ancestors(graph, target_node)
    # Use reversed graph for upstream traversal
    return set(nx.bfs_tree(graph.reverse(copy=False), target_node, depth_limit=depth_limit).nodes()) - {target_node}

def trace_downstream_paths(
    graph: nx.DiGraph,
    source_node: str,
    logger: lg.Logger,
    depth_limit: Optional[int] = None,
    target_node: Optional[str] = None
) -> List[List[str]]:
    """
    Trace all simple execution paths downstream from a selected node in the dependency graph.
    Paths can be limited by depth or traced to a specific target node.

    Args:
        graph: The NetworkX DiGraph representing dependencies.
        source_node: The node from which to start tracing.
        logger: A Loguru logger instance.
        depth_limit: Optional maximum path length (number of edges).
        target_node: Optional target node to trace paths to.

    Returns:
        A list of paths (each path is a list of node IDs). Returns an empty list if no paths found or nodes are missing.
    """
    logger.info(f"Tracing downstream paths from '{source_node}' (depth_limit={depth_limit}, target_node={target_node})...")
    if not graph:
        logger.warning("Graph is empty or None. Cannot trace downstream paths.")
        return []
    if source_node not in graph:
        logger.warning(f"Source node '{source_node}' not found in graph.")
        return []
    if target_node is not None and target_node not in graph:
        logger.warning(f"Target node '{target_node}' not found in graph.")
        return []

    try:
        if target_node:
            # Use all_simple_paths with cutoff if provided
            logger.debug(f"Tracing all simple paths from '{source_node}' to '{target_node}' (cutoff={depth_limit})...")
            paths = list(nx.all_simple_paths(graph, source=source_node, target=target_node, cutoff=depth_limit))
            logger.info(f"Found {len(paths)} paths from '{source_node}' to '{target_node}'.")
            return paths
        else:
            # No target: find all simple paths from source to all reachable nodes (up to depth_limit)
            # Use DFS to enumerate all simple paths up to depth_limit (path length = number of edges)
            def dfs(current_path: List[str], depth: int):
                current_node = current_path[-1]
                if depth_limit is not None and depth >= depth_limit:
                    return
                for neighbor in graph.successors(current_node):
                    if neighbor in current_path:
                        continue  # avoid cycles
                    new_path = current_path + [neighbor]
                    paths_found.append(new_path)
                    dfs(new_path, depth + 1)
            paths_found: List[List[str]] = []
            dfs([source_node], 0)
            logger.info(f"Found {len(paths_found)} downstream paths from '{source_node}'.")
            return paths_found
    except Exception as e:
        logger.error(f"Error tracing downstream paths from '{source_node}': {e}", exc_info=True)
        return []

def classify_nodes(
    graph: nx.DiGraph,
    logger: lg.Logger,
    complexity_metrics_available: bool = False,
    hub_degree_percentile: float = 0.95,
    hub_betweenness_percentile: float = 0.95,
    hub_pagerank_percentile: float = 0.95,
    utility_out_degree_percentile: float = 0.90,
    utility_max_complexity: int = 50,
    orphan_component_max_size: int = 4
) -> nx.DiGraph:
    """
    Classifies nodes in the dependency graph into architectural roles: Hubs, Utilities, Orphans, etc.
    Adds a 'node_role' attribute (list of roles) to each node.

    Args:
        graph: The NetworkX DiGraph to classify.
        logger: A Loguru logger instance.
        complexity_metrics_available: If True, use node complexity for utility classification.
        hub_degree_percentile: Percentile for degree threshold (default: 95th).
        hub_betweenness_percentile: Percentile for betweenness threshold (default: 95th).
        hub_pagerank_percentile: Percentile for PageRank threshold (default: 95th).
        utility_out_degree_percentile: Percentile for utility out-degree (default: 90th).
        utility_max_complexity: Max complexity for utility nodes (if available).
        orphan_component_max_size: Max size for a component to be considered orphaned.
    """
    import numpy as np
    logger.info("Classifying nodes by architectural role...")
    # --- Degree metrics ---
    degrees = dict(graph.degree())
    in_degrees = dict(graph.in_degree())
    out_degrees = dict(graph.out_degree())
    # --- Centrality metrics ---
    betweenness = nx.betweenness_centrality(graph, normalized=True)
    pagerank = nx.pagerank(graph)
    # --- Connected components ---
    wccs = list(nx.weakly_connected_components(graph))
    largest_wcc = max(wccs, key=len) if wccs else set()
    # --- Thresholds (percentile-based) ---
    degree_values = np.array(list(degrees.values()))
    betweenness_values = np.array(list(betweenness.values()))
    pagerank_values = np.array(list(pagerank.values()))
    out_degree_values = np.array(list(out_degrees.values()))
    hub_degree_thresh = np.percentile(degree_values, hub_degree_percentile * 100)
    hub_betweenness_thresh = np.percentile(betweenness_values, hub_betweenness_percentile * 100)
    hub_pagerank_thresh = np.percentile(pagerank_values, hub_pagerank_percentile * 100)
    utility_out_degree_thresh = np.percentile(out_degree_values, utility_out_degree_percentile * 100)
    # --- Complexity metrics (if available) ---
    node_complexity = {}
    if complexity_metrics_available:
        for node_id, data in graph.nodes(data=True):
            node_complexity[node_id] = data.get('acc', 0)  # Use 'loc' as a proxy
    # --- Assign roles ---
    for node_id in graph.nodes():
        roles = []
        # Hubs/connectors
        if (
            degrees[node_id] >= hub_degree_thresh or
            betweenness[node_id] >= hub_betweenness_thresh or
            pagerank[node_id] >= hub_pagerank_thresh
        ):
            roles.append('hub')
        # Utility nodes
        if out_degrees[node_id] >= utility_out_degree_thresh:
            if complexity_metrics_available:
                if node_complexity.get(node_id, 0) <= utility_max_complexity:
                    roles.append('utility')
            else:
                roles.append('utility')

        # Orphaned component members        
        for comp in wccs:
            if node_id in comp and len(comp) <= orphan_component_max_size and comp != largest_wcc:
                roles.append('orphan_component_member')
                break

        # Entry/terminal points (reuse existing logic)
        if in_degrees[node_id] == 0:
                roles.append('entry_point')    
        if out_degrees[node_id] == 0:
            roles.append('terminal_node')

        graph.nodes[node_id]['node_role'] = ", ".join(roles) if roles else []
        logger.debug(f"Node {node_id}: roles={roles}")
    logger.info("Node classification complete.")

    return graph

def list_nodes(
    graph: nx.DiGraph,
    logger: lg.Logger,
    filter_node_type: List[str] = [],
    filter_packages: List[str] = [],
    filter_name_substr: Optional[str] = None,
    limit: Optional[int] = None,
    sort_by: str = "name"
) -> List[Dict[str, str]]:
    """
    List all nodes in the graph with optional filtering and sorting.
    
    Args:
        graph: The NetworkX DiGraph
        logger: Logger instance
        filter_node_type: Filter by code object type (case-insensitive)
        filter_packages: Filter by package name (case-insensitive substring match)
        filter_name_substr: Filter by node name (case-insensitive substring match)
        limit: Maximum number of nodes to return
        sort_by: Sort field ('name', 'type', 'package', 'degree')
        
    Returns:
        List of dictionaries containing node information
    """
    if not graph:
        logger.warning("Graph is empty or None. Cannot list nodes.")
        return []
    
    logger.info(f"Listing nodes with filters - type: {filter_node_type}, package: {filter_packages}, name: {filter_name_substr}")
    
    nodes_info = []
    
    # Extract information for each node
    for node_id, node_data in graph.nodes(data=True):

        # Extract basic information
        node_info = {
            'id': node_id,
            'name': node_data['name'],
            'type': node_data.get('type', 'UNKNOWN').upper(),
            'package': node_data['package_name'] if 'package_name' in node_data else '',
            'loc': node_data.get('loc', None),  # Lines of Code
            'parameters': node_data.get('num_params', None),  # Number of parameters
            'calls-made': node_data.get('num_calls_made', None),  # Number of outgoing calls
            'called-by': graph.in_degree(node_id),
            'acc': node_data.get('acc', None)  # Approximate Cyclomatic Complexity
        }
        
        # Apply filters
        filter_node_type = [ftype.upper() for ftype in filter_node_type]  # Normalize to uppercase
        if filter_node_type and node_info['type'] not in filter_node_type:
            logger.debug(f"Node '{node_id}' of type '{node_info['type']}' filtered out by type.")
            continue
        
        filter_packages = [pkg.casefold() for pkg in filter_packages]  # Normalize to lowercase
        if filter_packages and node_info['package'].casefold() not in filter_packages:
            logger.debug(f"Node '{node_id}' in package '{node_info['package']}' filtered out by package.")
            continue
            
        if filter_name_substr and filter_name_substr.casefold() not in node_info['name'].casefold():
            logger.debug(f"Node '{node_id}' with name '{node_info['name']}' filtered out by name substring '{filter_name_substr}'.")
            continue
        
        nodes_info.append(node_info)
    
    # Sort nodes
    if sort_by == "id":
        nodes_info.sort(key=lambda x: x['id'].lower())
    elif sort_by == "name":
        nodes_info.sort(key=lambda x: x['name'].lower())
    elif sort_by == "type":
        nodes_info.sort(key=lambda x: (x['type'], x['name'].lower()))
    elif sort_by == "package":
        nodes_info.sort(key=lambda x: (x['package'].lower(), x['name'].lower()))
    elif sort_by == "degree":
        nodes_info.sort(key=lambda x: (x['in_degree'] + x['out_degree'], x['name'].lower()), reverse=True)
    elif sort_by == "acc":
        nodes_info.sort(key=lambda x: (x['acc'] if x['acc'] is not None else float('inf'), x['name'].lower()))
    else:
        logger.warning(f"Unknown sort field '{sort_by}', defaulting to name")
        nodes_info.sort(key=lambda x: x['name'].lower())
    
    # Apply limit
    if limit == 0:
        logger.warning("Limit is set to 0, returning no nodes.")
        return []
    if limit and limit > 0:
        nodes_info = nodes_info[:limit]
    
    logger.info(f"Returning {len(nodes_info)} nodes after filtering and sorting")
    return nodes_info

def analyze_cycles_enhanced(
    graph: nx.DiGraph,
    logger: lg.Logger,
    min_cycle_length: Optional[int] = None,
    max_cycle_length: Optional[int] = None,
    sort_by: str = "length",
    include_node_details: bool = False
) -> List[Dict]:
    """
    Find and analyze circular dependencies with advanced filtering and sorting.
    
    Args:
        graph: The NetworkX DiGraph
        logger: Logger instance
        min_cycle_length: Filter cycles with minimum length
        max_cycle_length: Filter cycles with maximum length
        sort_by: Sort cycles by 'length', 'nodes', or 'complexity'
        include_node_details: Include detailed node information
        
    Returns:
        List of dictionaries containing cycle information
    """
    logger.info("Analyzing circular dependencies with enhanced filtering...")
    
    if not graph:
        logger.warning("Graph is empty or None. Cannot analyze cycles.")
        return []

    try:
        # Get all cycles using the existing function
        cycles_raw = find_circular_dependencies(graph, logger)
        
        if not cycles_raw:
            logger.info("No circular dependencies found.")
            return []

        cycles_info = []
        
        for i, cycle in enumerate(cycles_raw):
            cycle_length = len(cycle)
            
            # Apply length filters
            if min_cycle_length is not None and cycle_length < min_cycle_length:
                continue
            if max_cycle_length is not None and cycle_length > max_cycle_length:
                continue
            
            # Calculate complexity (sum of node degrees)
            complexity = sum(graph.degree(node) for node in cycle if node in graph)
            
            cycle_info = {
                'cycle_id': i + 1,
                'nodes': cycle,
                'length': cycle_length,
                'complexity': complexity,
                'cycle_path': ' → '.join(cycle) + f' → {cycle[0]}'
            }
            
            # Add detailed node information if requested
            if include_node_details:
                node_details = []
                for node in cycle:
                    if node in graph:
                        node_data = graph.nodes[node]
                        detail = {
                            'id': node,
                            'name': node_data.get('name', node),
                            'type': node_data.get('type', 'UNKNOWN'),
                            'package': node_data.get('package_name', ''),
                            'in_degree': graph.in_degree(node),
                            'out_degree': graph.out_degree(node)
                        }
                        node_details.append(detail)
                cycle_info['node_details'] = node_details
            
            cycles_info.append(cycle_info)
        
        # Sort cycles
        if sort_by == "length":
            cycles_info.sort(key=lambda x: x['length'])
        elif sort_by == "nodes":
            cycles_info.sort(key=lambda x: x['nodes'][0])  # Sort by first node ID
        elif sort_by == "complexity":
            cycles_info.sort(key=lambda x: x['complexity'], reverse=True)
        
        logger.info(f"Found {len(cycles_info)} cycles after filtering (from {len(cycles_raw)} total)")
        
        return cycles_info
        
    except Exception as e:
        logger.error(f"Error analyzing cycles: {e}", exc_info=True)
        return []
